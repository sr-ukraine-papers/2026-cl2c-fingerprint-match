<!doctype html>
<html>

<head>
<meta charset="utf-8">

<!-- Proprietary CSS -->
<!-- <link href="style.css" rel="stylesheet"> -->

<!-- Google fonts -->
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Cantarell">

<!-- Bootstrap core CSS -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/5.0.0-alpha1/css/bootstrap.min.css"
		integrity="sha384-r4NyP46KrjDleawBgD5tp8Y7UzmLA05oM1iAEQ17CSuDqnUK2+k9luXQOfXJCJ4I" crossorigin="anonymous">

<!-- Plotly.js for 3D visualization -->
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

<!-- Care about mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Contactless-to-contact fingerprint matching on smartphones</title>

</head>

<body>

<div class="container w-50">

	<div class="row text-center">
		<div class="col-12">
		<h1 class="mt-5">Robust user authentication on mobile devices with dynamic 3D fingerprint representation</h1>
		<h5 class="text-muted my-4">IEEE Transactions on Information Forensics and Security</h5>
		<!-- <h5 class="my-4">Who Are You?! Adventures in Authentication Workshop, WAY-2021, Vancouver, B.C., Canada, August 11-13, 2021</h5> -->
		</div>
	</div>

	<div class="row text-center">
		<div class="col-6">
			<a type="button" class="btn btn-primary" href="mailto:mage.kim@samsung.com">
				<img src="images/at.svg"/>
				Minho Kim</a>
			<div class="col">
			<a type="button" class="btn btn-link" href="https://www.samsung.com/us/">Samsung Electronics, Mobile Experience R&D Center</a>
			</div>
		</div>
		<div class="col-6">
			<a type="button" class="btn btn-primary" href="mailto:sohyeon.jeon@samsung.com">
				<img src="images/at.svg"/>
				Sohyeon Jeon</a>
			<div class="col">
			<a type="button" class="btn btn-link" href="https://www.samsung.com/us/">Samsung Electronics, Mobile Experience R&D Center</a>
			</div>
		</div>
		<div class="col-6">
			<a type="button" class="btn btn-primary" href="mailto:hosang21.lee@samsung.com">
				<img src="images/at.svg"/>
				Hosang Lee</a>
			<div class="col">
			<a type="button" class="btn btn-link" href="https://www.samsung.com/us/">Samsung Electronics, Mobile Experience R&D Center</a>
			</div>
		</div>
		<div class="col-6">
			<a type="button" class="btn btn-primary" href="mailto:moons.chang@samsung.com">
				<img src="images/at.svg"/>
				Moonsoo Chang</a>
			<div class="col">
			<a type="button" class="btn btn-link" href="https://www.samsung.com/us/">Samsung Electronics, Mobile Experience R&D Center</a>
			</div>
		</div>
		<div class="col-6">
			<a type="button" class="btn btn-primary" href="mailto:d.progonov@samsung.com">
				<img src="images/at.svg"/>
				Dmytro Progonov</a>
			<div class="col">
			<a type="button" class="btn btn-link" href="https://research.samsung.com/srk">Samsung R&D Institute Ukraine</a>
			</div>
			<div class="col">
			<a type="button" class="btn btn-link" href="https://kpi.ua/en/">Igor Sikorsky Kyiv Polytechnic Institute</a>
			</div>
		</div>
		<div class="col-6">
			<a type="button" class="btn btn-primary" href="mailto:va.kuznetsov@samsung.com">
				<img src="images/at.svg"/>
				Vasyl Kuznetsov</a>
			<div class="col">
			<a type="button" class="btn btn-link" href="https://research.samsung.com/srk">Samsung R&D Institute Ukraine</a>
			</div>
		</div>
	</div>

	<hr/>

	<div class="row text-center">
		<div class="col-4">
			<a type="button" class="btn btn-light" href="TBD">
				<img src="images/file-pdf.svg" class="img-fluid p-2" alt="file">
				<h5>Paper</h5>
				<p>Download pdf</p>
			</a>
		</div>
		<div class="col-4">
			<a type="button" class="btn btn-light" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=10206">
				<img src="images/person-chalkboard.svg" class="img-fluid p-2" alt="file">
				<h5>IEEE TIFS</h5>
				<p>Conference</p>
			</a>
		</div>
		<div class="col-4">
			<a type="button" class="btn btn-light" href="./bibtex_citation.txt">
				<img src="images/graduation-cap.svg" class="img-fluid p-2" alt="file">
				<h5>BIBTEX</h5>
				<p>Cite the paper</p>
			</a>
		</div>
	</div>
    
	<div class="row">
	<div class="col-12">
		<h4 class="text-center mt-4">Abstract</h4>
	</div>

            <div class="offset-md-0 col-md-12 main_text">
              <div class="lead mb-0">
                Modern mobile devices include a wide range of sensors for gathering and processing biometric data. 
				Of special interest is fingerprint sensors due to their low cost, high energy efficiency, and availability 
				of fast methods for fingerprint samples matching. Nevertheless, robust matching of users' fingerprint 
				in various conditions (e.g. wet or damaged skin) during authentication remains an unsolved task. 
				This task is complicated by the necessity of matching samples created using different methods, such as 
				a contact-based template with a new contactless fingerprint sample.

				<p>&nbsp;</p>

				This work presents methods for reliable fingerprint recognition and matching irrespective of the fingerprint 
				retrieval approach used. We proposed using a unified 3D-fingerprint model that preserves fine structure instead of 
				creating contact-based or contactless samples during user enrollment. Fast methods for reliable restoration of 
				such fingerprints and their adaptation for matching with other samples are proposed. Performance analysis of 
				the state-of-the-art and proposed solution proved its efficiency in a wide range of practical use cases. 
				The user authentication accuracy (FAR=3.08%, FRR=2.67%) demonstrates up to six times improvement 
				over state-of-the-art solutions, even for the most difficult case of contactless-to-contactless fingerprint matching. 
				Thus, the proposed solution enables reliable user authentication by matching with both contactless and 
				contact-based fingerprint samples without needing to store a set of templates. This is especially important 
				for resource-constrained smartphones and other mobile devices with built-in fingerprint sensors.

				<p>&nbsp;</p>

				</div>
            </div>
            <p></p>
          </div>
		  
	<div class="row">
	<div class="col-12">
		<h4 class="text-center mt-4">Proposed solution</h4>
	</div>

            <div class="offset-md-0 col-md-12 main_text">
              <div class="lead mb-0">
                Our method uses a unified 3D-fingerprint that preserves the fine structure of fingerprint ridges 
				as well as the user's finger shape (namely, fingertip surface curvature and geometric proportions). 
				Then, the 3D-fingerprint can be used for matching with either contactless or contact-based templates 
				by unwrapping into 2D plane. Optionally, the restored fingerprint may be compared with other 3D-fingerprints 
				gathered with specialized hardware.

				<p>&nbsp;</p>

				The accurate restoration of the user's fingerprint by the proposed solution is achieved through 
				the integration of novel methods for 3D-fingerprint reconstruction and their robust matching 
				with templates obtained in contact-based and contactless manners. The main processing stages 
				of the proposed solution are presented below.

				<p>&nbsp;</p>
				
				<table border="1" cellpadding="10" cellspacing="10" style="min-width:500px" class="mx-auto">
					<tbody>
						<tr>
							<td style="text-align:center"><img width=100% height=100% src="images/MainPipeline2.png" type="image/png" alt="Main-Pipeline-2" object-fit=contain></td>
						</tr>
					</tbody>
				</table>
				
				<p>&nbsp;</p>
				
				The contactless fingerprint restoration begins with gathering a pair of fingertip surface images 
				using the built-in rear camera of a smartphone. We use the widespread assumption that images are 
				gathered under uniform lighting, and the camera's intrinsic parameters (such as focus distance and 
				optical center position) are known in advance. Thus, no additional calibration of the used camera is needed.
				
				<p>&nbsp;</p>
				
				At the second stage, gathered pairs of images are used for restoring the 3D-fingerprint. A part (slice) of 
				the whole fingertip surface is reconstructed from each captured image. Then, these slices are merged 
				into a single 3D-fingerprint. The unwrapping procedure is applied to convert the obtained 3D-fingerprint 
				into a planar one and to make fingerprint matching with the reference template possible. 
				The distinctive feature of the proposed unwrapping method is taking into account the curvature and 
				orientation of the processed 3D-fingerprint to minimize elastic and perspective distortions. 
				Finally, the matcher module processes both the prepared fingerprint and the reference template to obtain 
				the final decision for user authentication.

				</div>
            </div>
            <p></p>
          </div>
    
	<div class="row">
	<div class="col-12">
		<h4 class="text-center mt-4">Experiment results</h4>
	</div>

            <div class="offset-md-0 col-md-12 main_text">
              <div class="lead mb-0">
                The evaluation of state-of-the-art and proposed solutions was done on an in-house collected dataset. 
				The dataset includes fingerprint samples collected from 50 volunteers (31 males and 19 females). 
				The fingerprint samples for each person were gathered for 10 fingers rotated to the left, right, and top directions 
				from 0° to 45° degrees with a step of 15° degrees. Image gathering was repeated 5 times 
				for each finger and each type of rotation. A lightbox with a color temperature of 5,500 (K) was used to 
				provide uniform lighting during dataset preparation. The dataset collection procedure follows the requirements 
				of <a href="https://www.iso.org/standard/73515.html">ISO/IEC 19795-1:2021</a> 
				and <a href="https://www.nist.gov/publications/data-format-interchange-fingerprint-facial-other-biometric-information-ansinist-itl-1-1">NIST SP 500-290</a> standards.

				<p>&nbsp;</p>

				The high-resolution fingerprint scanner Supreme RealScan-D was used to collect the reference fingerprint samples. 
				Also, the popular smartphone models Samsung Galaxy SM-A146B and Galaxy SM-A256B were applied to gather images for 
				contactless fingerprint restoration. The statistics of the collected samples are presented in the table below.

				<p>&nbsp;</p>
				
				<style>
					table {
						border-collapse: collapse;
						width: 100%;
					}
					th, td {
						border: 1px solid black;
						padding: 10px;
						text-align: center;
						vertical-align: middle;
					}
				</style>

				<table>
					<!-- Header Row 1 -->
					<tr>
						<th colspan="3" rowspan="2">Used sensor</th>
						<th colspan="2">Number of samples</th>
					</tr>
					
					<!-- Header Row 2 -->
					<tr>
						<th>In total</th>
						<th>Per user<br>in average</th>
					</tr>
					
					<!-- Data Row 1: Reference Scanner -->
					<tr>
						<td colspan="3">Reference scanner Supreme RealScan-D</td>
						<td>5,000</td>
						<td>100</td>
					</tr>
					
					<!-- Smartphones Row 1: SM-A146B from rear camera -->
					<tr>
						<td rowspan="4">Smart-<br>phones</td>
						<td rowspan="2">Samsung<br>SM-A146B</td>
						<td>from rear camera</td>
						<td>53,814</td>
						<td>1,076</td>
					</tr>
					
					<!-- Smartphones Row 2: SM-A146B from SMFS -->
					<tr>
						<td>from SMFS</td>
						<td>39,414</td>
						<td>1,577</td>
					</tr>
					
					<!-- Smartphones Row 3: SM-A256B from rear camera -->
					<tr>
						<td rowspan="2">Samsung<br>SM-A256B</td>
						<td>from rear camera</td>
						<td>64,924</td>
						<td>1,298</td>
					</tr>
					
					<!-- Smartphones Row 4: SM-A256B from SMFS -->
					<tr>
						<td>from SMFS</td>
						<td>39,414</td>
						<td>1,577</td>
					</tr>
				</table>
				
				<p>&nbsp;</p>
				
				We performed a modeling study to justify that our 3D reconstruction procedure is precise 
				enough for user authentication. Due to the absence of a "normal" (standard) 3D shape for 
				the human fingertip surface in the open literature, we applied an ellipsoid surface for its approximation. 
				The parameters of the used ellipsoid were randomly selected from a uniform distribution to model the size of real fingers:

				<ul>
					<li>The length for OX axis (width of a finger) was chosen randomly around 18 (mm), allowing deviation of ±25%;</li>
					<li>The length for OY axis (height of a finger) was chosen randomly around 18 (mm), allowing deviation of ±25%;</li>
					<li>The lengths for OZ half-axis (depth of a finger) was chosen around 9 (mm), allowing deviation of ±25%.</li>
				</ul>
				
				The images gathering procedure was emulated using pinhole camera model. We considered the case of pinhole cameras placed 
				in two positions selected according to the table:
				
				<p>&nbsp;</p>
				
				<style>
					table {
						border-collapse: collapse;
						width: 100%;
					}
					th, td {
						border: 1px solid black;
						padding: 10px;
						text-align: center;
						vertical-align: middle;
					}
					th {
						font-weight: bold;
					}
				</style>

				<table>
					<!-- Header Row -->
					<tr>
						<th></th>
						<th>First camera</th>
						<th>Second camera</th>
					</tr>
					
					<!-- Data Row 1 -->
					<tr>
						<td>along OX axis, mm</td>
						<td>x<sub>1</sub> ~ [-40, -10]</td>
						<td>x<sub>2</sub> ~ [10, 40]</td>
					</tr>
					
					<!-- Data Row 2 -->
					<tr>
						<td>along OY axis, mm</td>
						<td>y<sub>1</sub> ~ [-10, -10]</td>
						<td>y<sub>2</sub> ~ [-10, 10]</td>
					</tr>
					
					<!-- Data Row 3 -->
					<tr>
						<td>along OZ axis, mm</td>
						<td>z<sub>1</sub> ~ [90, 110]</td>
						<td>z<sub>2</sub> ~ [90, 110]</td>
					</tr>
					
					<!-- Data Row 4 -->
					<tr>
						<td>Euler rotation angles,<br>degrees</td>
						<td>(α<sub>1</sub>, α<sub>2</sub>, α<sub>3</sub>):<br>α<sub>i</sub> = 0°</td>
						<td>(α<sub>1</sub>', α<sub>2</sub>', α<sub>3</sub>'):<br>α<sub>i</sub>' ~ [-25°, 25°]</td>
					</tr>
				</table>
				
				<p>&nbsp;</p>
				
				In figure below, we show an example of the distribution of ground truth points on an ellipsoid surface. 
				We placed 200 randomly selected points on the ellipsoid surface and calculated their projections onto visible 
				2D planes of fields of view. The 2D points projected from same 3D-point are referred as corresponding ones. 
				Finally, the coordinates of the obtained 2D points were rounded to integers (pixels). This rounding introduced 
				some errors into the point coordinates, and the larger the focal length, the smaller this noise will be.
				
				<p>&nbsp;</p>
				
				<div class="container">
					<div class="controls mb-4" style="display: flex; justify-content: center; gap: 20px; flex-wrap: wrap;">
						<div class="control-group" style="display: flex; flex-direction: column; align-items: center;">
							<label for="axisA" style="font-weight: bold; margin-bottom: 5px;">Half-axis A (X)</label>
							<input type="number" id="axisA" value="9.0" step="0.1" min="0.1" style="width: 80px; padding: 8px; border: 1px solid #ddd; border-radius: 4px; text-align: center;">
						</div>
						<div class="control-group" style="display: flex; flex-direction: column; align-items: center;">
							<label for="axisB" style="font-weight: bold; margin-bottom: 5px;">Half-axis B (Y)</label>
							<input type="number" id="axisB" value="14.0" step="0.1" min="0.1" style="width: 80px; padding: 8px; border: 1px solid #ddd; border-radius: 4px; text-align: center;">
						</div>
						<div class="control-group" style="display: flex; flex-direction: column; align-items: center;">
							<label for="axisC" style="font-weight: bold; margin-bottom: 5px;">Half-axis C (Z)</label>
							<input type="number" id="axisC" value="6.0" step="0.1" min="0.1" style="width: 80px; padding: 8px; border: 1px solid #ddd; border-radius: 4px; text-align: center;">
						</div>
						<button onclick="updateEllipsoidPlot()" class="btn btn-primary" style="align-self: center;">Update Plot</button>
					</div>
					<p class="text-center text-muted">Drag to rotate | Scroll to zoom</p>
					<div id="ellipsoidPlot" style="width: 100%; height: 500px; background-color: white; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1);"></div>
				</div>
				
				<p>&nbsp;</p>
				
				<style>
					.math-variable {
						font-style: italic;
						font-family: 'Times New Roman', serif;
					}
					.citation {
						color: #0066cc;
					}
				</style>

				<p>
					Several state-of-the-art methods for fundamental matrix <span class="math-variable">F</span> estimation were checked, 
					namely RANSAC, 7Point/8Point<sup class="citation">[Hartley:MultipleView]</sup>, and USAC<sup class="citation">[Raguram:USAC]</sup>. 
					During the evaluation, we used the default parameters for these methods according to the reference implementation in 
					the OpenCV library<sup class="citation">[OpenCV:USAC]</sup>. 
					In Figure 1: Reconstruction Error, we show the dependency of the RMSE of reconstructed point positions 
					and rotation errors on the camera focal length.
				</p>
				
				<p>&nbsp;</p>
				
				<!-- Reconstruction Error Plots -->
				<div class="container">
					<h5 class="text-center">Interactive Reconstruction Error Analysis</h5>
					
					<!-- Phone Selector -->
					<div class="row mb-3">
						<div class="col-12">
							<label for="phone-select" style="font-weight: bold;">Target Phone:</label>
							<select id="phone-select" class="form-select" onchange="updateReconstructionPhoneSelection()" style="max-width: 300px;">
								<option value="s928b">s928b (2830 pixels)</option>
								<option value="a146b" selected>a146b (3000 pixels)</option>
							</select>
						</div>
					</div>
					
					<!-- Position Errors Plot -->
					<div class="row mb-4">
						<div class="col-12">
							<h6 class="text-center">Position Errors</h6>
							<div class="card p-3 mb-3">
								<div class="checkbox-container mb-3" style="display: flex; flex-wrap: wrap; gap: 15px;">
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="pos-RANSAC" checked onchange="updateReconstructionPositionPlot()">
										<label class="form-check-label" for="pos-RANSAC">RANSAC</label>
									</div>
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="pos-7Point" checked onchange="updateReconstructionPositionPlot()">
										<label class="form-check-label" for="pos-7Point">7Point</label>
									</div>
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="pos-8Point" checked onchange="updateReconstructionPositionPlot()">
										<label class="form-check-label" for="pos-8Point">8Point</label>
									</div>
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="pos-USAC_DEFAULT" checked onchange="updateReconstructionPositionPlot()">
										<label class="form-check-label" for="pos-USAC_DEFAULT">USAC_DEFAULT</label>
									</div>
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="pos-USAC_ACCURATE" checked onchange="updateReconstructionPositionPlot()">
										<label class="form-check-label" for="pos-USAC_ACCURATE">USAC_ACCURATE</label>
									</div>
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="pos-USAC_MAGSAC" checked onchange="updateReconstructionPositionPlot()">
										<label class="form-check-label" for="pos-USAC_MAGSAC">USAC_MAGSAC</label>
									</div>
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="pos-USAC_PROSAC" checked onchange="updateReconstructionPositionPlot()">
										<label class="form-check-label" for="pos-USAC_PROSAC">USAC_PROSAC</label>
									</div>
								</div>
								<div id="reconstructionPositionChart" style="width: 100%; height: 400px;"></div>
							</div>
						</div>
					</div>
					
					<!-- Angle Errors Plot -->
					<div class="row mb-4">
						<div class="col-12">
							<h6 class="text-center">Angle Errors</h6>
							<div class="card p-3">
								<div class="checkbox-container mb-3" style="display: flex; flex-wrap: wrap; gap: 15px;">
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="ang-RANSAC" checked onchange="updateReconstructionAnglePlot()">
										<label class="form-check-label" for="ang-RANSAC">RANSAC</label>
									</div>
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="ang-7Point" checked onchange="updateReconstructionAnglePlot()">
										<label class="form-check-label" for="ang-7Point">7Point</label>
									</div>
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="ang-8Point" checked onchange="updateReconstructionAnglePlot()">
										<label class="form-check-label" for="ang-8Point">8Point</label>
									</div>
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="ang-USAC_DEFAULT" checked onchange="updateReconstructionAnglePlot()">
										<label class="form-check-label" for="ang-USAC_DEFAULT">USAC_DEFAULT</label>
									</div>
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="ang-USAC_ACCURATE" checked onchange="updateReconstructionAnglePlot()">
										<label class="form-check-label" for="ang-USAC_ACCURATE">USAC_ACCURATE</label>
									</div>
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="ang-USAC_MAGSAC" checked onchange="updateReconstructionAnglePlot()">
										<label class="form-check-label" for="ang-USAC_MAGSAC">USAC_MAGSAC</label>
									</div>
									<div class="form-check">
										<input class="form-check-input" type="checkbox" id="ang-USAC_PROSAC" checked onchange="updateReconstructionAnglePlot()">
										<label class="form-check-label" for="ang-USAC_PROSAC">USAC_PROSAC</label>
									</div>
								</div>
								<div id="reconstructionAngleChart" style="width: 100%; height: 400px;"></div>
							</div>
						</div>
					</div>
				</div>
				
				<p>&nbsp;</p>

				<p>
					The proposed solution was compared with state-of-the-art methods of contact-to-contactless fingerprint matching. The performance evaluation was done using FAR and FRR according to the recommendations for measuring biometric unlock security for Android OS<sup class="citation">[Android:MeasuringBiometricUnlockSecurity]</sup>.
				</p>

				<p>
					During the evaluation, we compared the proposed solution with the following methods:
				</p>

				<ul>
					<li><strong>VeriFinger SDK</strong> — is based on extraction and matching of popular fingerprint features, namely minutiae, on fingerprint samples<sup class="citation">[Verifinger:SDK]</sup>. The trial version of SDK was used due to licensing restrictions.</li>
					
					<li><strong>Grosz et al. method</strong> — is based on matching the contactless-to-contact based fingerprint using minutiae and texture features<sup class="citation">[Grosz:C2CL]</sup>. The method includes TPS module to detect elastic distortions on contactless fingerprint and to reduce their impact on matching accuracy. The texture-related features are extracted using modern DeepPrint network proposed by Engelsma et al.<sup class="citation">[Engelsma:DeepPrint]</sup>. The minutiae are extracted using popular VeriFinger SDK<sup class="citation">[Verifinger:SDK]</sup>. Matching of fingerprint was done by comparison of individual matching of texture-related features and minutiae according to authors recommendations<sup class="citation">[Grosz:C2CL]</sup>.</li>
					
					<li><strong>Pan et al. method</strong> — is based on applying the original deep-learning based dense minutiae descriptor<sup class="citation">[Pan:LatentFingerprintMatching]</sup>. It is represented as 3D-array, with two dimensions associated with the original image plane and the other dimension representing the abstract features. The proposed solution allows for reliable fingerprints matching while preserving relatively low computational overhead. We re-used the open source implementation of proposed solution<sup class="citation">[Pan:LatentFingerprintMatchingImplementation]</sup>.</li>
					
					<li><strong>MinNet model</strong> — is proposed by Ozturk et al.<sup class="citation">[Ozturk:MinNet]</sup>, and is based on using the minutiae patch embedding network. Embedding vectors generated for a fixed-size patch extracted around a minutiae are used in the local similarity assignment algorithm to produce a global similarity. This makes possible joint optimization of the spatial and angular distribution of neighboring minutiae and ridge flows of the patches. Due to absent of publicly available implementation, we re-implemented the solution and performed its training on collected in-house dataset.</li>
				</ul>

				<p>
					The performance evaluation was done for three cases. First, we analyzed the accuracy of the restored contactless fingerprint by matching it with samples from the contact-based reference fingerprint scanner. Then, we considered a perspective scenario where the reference template is created on-device in a contactless manner, while user authentication is performed by gathering samples from the built-in fingerprint scanner. The last case relates to the situation of matching both the template and the gathered sample contactlessly. In this case, we considered the performance of the solution through the use of a mobile device that has only a camera and does not include any built-in fingerprint sensors.
				</p>

				<p>
					The estimated authentication accuracy for the proposed and state-of-the-art solutions for the first considered use case (matching of contactless and contact fingerprints) is presented in Table 1: MatchContactlessToContactMerged.
				</p>

				<p class="caption">User authentication accuracy by matching of restored fingerprint with reference template using in-house dataset. The results are presented in the format of mean ± standard deviation.</p>

				<table>
					<!-- Header Row 1 -->
					<tr>
						<th rowspan="2">Processing method</th>
						<th colspan="2">Contactless fingerprint to<br>contact template matching</th>
						<th colspan="2">Contact fingerprint to<br>contactless template matching</th>
						<th colspan="2">Contactless fingerprint to<br>contactless template matching</th>
					</tr>
					
					<!-- Header Row 2 -->
					<tr>
						<th>FAR metric</th>
						<th>FRR metric</th>
						<th>FAR metric</th>
						<th>FRR metric</th>
						<th>FAR metric</th>
						<th>FRR metric</th>
					</tr>
					
					<!-- Data Row 1 -->
					<tr>
						<td>Ideal case</td>
						<td>0.00</td>
						<td>0.00</td>
						<td>0.00</td>
						<td>0.00</td>
						<td>0.00</td>
						<td>0.00</td>
					</tr>
					
					<!-- Data Row 2 -->
					<tr>
						<td>VeriFinger SDK</td>
						<td>28.13±12.96</td>
						<td>35.01±10.14</td>
						<td>23.45±9.23</td>
						<td>33.78±12.02</td>
						<td>48.48±15.74</td>
						<td>40.04±16.64</td>
					</tr>
					
					<!-- Data Row 3 -->
					<tr>
						<td>Grosz et al. method</td>
						<td>7.03±6.11</td>
						<td>8.31±7.22</td>
						<td>6.08±8.23</td>
						<td>7.93±9.14</td>
						<td>8.02±10.47</td>
						<td>9.04±12.27</td>
					</tr>
					
					<!-- Data Row 4 -->
					<tr>
						<td>Pan et al. method</td>
						<td>11.48±6.47</td>
						<td>14.21±1.82</td>
						<td>12.18±9.34</td>
						<td>12.57±6.28</td>
						<td>17.57±11.81</td>
						<td>18.76±10.78</td>
					</tr>
					
					<!-- Data Row 5 -->
					<tr>
						<td>MinNet model</td>
						<td>13.25±6.14</td>
						<td>18.32±7.58</td>
						<td>9.27±7.42</td>
						<td>15.24±8.27</td>
						<td>13.48±9.27</td>
						<td>20.18±9.08</td>
					</tr>
					
					<!-- Data Row 6 -->
					<tr>
						<th>Proposed solution</th>
						<td>2.05±1.37</td>
						<td>3.07±1.87</td>
						<td>1.93±2.15</td>
						<td>2.24±2.08</td>
						<td>3.08±1.99</td>
						<td>2.67±1.08</td>
					</tr>
				</table>
				
				<p>&nbsp;</p>
				
				<p>
					The results of experimental evaluation of modern and proposed methods proved the effectiveness of our solution in case of matching similar fingerprint types, namely contactless-to-contactless. The proposed solution preserves low error rate (within 3%), while applying state-of-the-art solutions leads to dramatically increasing of FAR and FRR up to six times (FAR ≃ 17.57% and FRR ≃ 18.76%). In the case of cross-matching of contactless fingerprints with contact-based fingerprints, the state-of-the-art methods allows to achieve moderate accuracy (FAR ≃ 11.48% and FRR ≃ 14.21%). The proposed solution makes possible decreasing the error rates up to five times (FAR ≃ 2.05% and FRR ≃ 3.07%). This proves effectiveness of our methods in the most difficult cases of fingerprints cross-matching with minimal impact on processing duration, where modern solutions requires thorough and tedious adaptation for each case.
				</p>

				</div>
            </div>
            <p></p>
          </div>
	
	</div>

	<script>
		// Phone focal lengths for reconstruction plots
		const reconstructionPhoneFocalLengths = {
			"s928b": 2830,
			"a146b": 3000
		};

		// Embedded data from position_errors.json
		const positionErrorsData = {
			"RANSAC": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "pos_error": [3.4776444108183546, 1.946396910062006, 1.3434797793745523, 0.9813715180718523, 0.7682782530151467, 0.6629600513833389, 0.5336751094960649, 0.46169650143784985, 0.40353715748503616, 0.3734966165505328]},
			"7Point": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "pos_error": [1.3277336151089483, 0.5913994644345356, 0.39875292309279603, 0.28738452058218267, 0.24638462778859221, 0.18960514903938191, 0.16042487320712115, 0.13875602457133876, 0.12196429515586917, 0.10799731542471883]},
			"8Point": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "pos_error": [0.5068085394975538, 0.23441876003003376, 0.16174702591534848, 0.10953294765570566, 0.08601016656702604, 0.07232018817088354, 0.06254520177353583, 0.05312690122555644, 0.04794956239837235, 0.04431267294087232]},
			"USAC_DEFAULT": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "pos_error": [0.5359945098406034, 0.23022018589219026, 0.15503716562539627, 0.11222717221118364, 0.08725654396321028, 0.07293525659587753, 0.06297456885961641, 0.05486759172663217, 0.048584988773771424, 0.04680668076860321]},
			"USAC_ACCURATE": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "pos_error": [0.5429001848809942, 0.23038494907371, 0.15583593156018594, 0.11210990405192878, 0.08772086254282489, 0.0728178877493593, 0.06326658971885214, 0.054610243175742604, 0.04953375315644325, 0.04458752067204961]},
			"USAC_MAGSAC": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "pos_error": [0.5142674042114168, 0.22924811646269638, 0.15761279767098724, 0.10918594307020812, 0.08772678647168128, 0.07569555056945061, 0.06512516694415021, 0.05426183031757396, 0.05031554810908931, 0.04519669897176993]},
			"USAC_PROSAC": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "pos_error": [0.5398653465551645, 0.23257229351440306, 0.16300571056700283, 0.11255053028740593, 0.0871748497787281, 0.07224601493366768, 0.06314481769419092, 0.0530529685056175, 0.04825095469944841, 0.04411934742100377]}
		};

		// Embedded data from angle_errors.json
		const angleErrorsData = {
			"RANSAC": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "angle_error": [17.621195418604316, 10.300695307883544, 7.424776659927814, 5.359917299297408, 4.343203538455478, 3.7791081679199094, 2.9795945739328977, 2.5749957200785967, 2.253238211169137, 2.2532165855474973]},
			"7Point": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "angle_error": [7.147709775926794, 3.2671728969105374, 2.1669470924792305, 1.5723909629908301, 1.278286582413675, 1.040525503714567, 0.8774884601942128, 0.7879565303376399, 0.7054028659075199, 0.6139856209582762]},
			"8Point": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "angle_error": [3.837733209452682, 1.6895729309698675, 1.2932204827253193, 0.8241996303548329, 0.6508196886788091, 0.5457491436740948, 0.4942371808620644, 0.4133743861448412, 0.37976896754709943, 0.3324481669804401]},
			"USAC_DEFAULT": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "angle_error": [3.8013744370829237, 1.6314046529378252, 1.070580639128386, 0.7896992410087069, 0.6422252555969654, 0.5337136290723533, 0.4696353744863381, 0.4034866531933959, 0.36174992301928866, 0.31787841678835854]},
			"USAC_ACCURATE": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "angle_error": [3.8109341306042213, 1.6299452533107153, 1.077220378553727, 0.7921351498058506, 0.6387979567853714, 0.533647089262823, 0.47395844115223834, 0.4034687743706804, 0.3559947049963643, 0.3107684599245186]},
			"USAC_MAGSAC": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "angle_error": [3.1845249211724425, 1.4456045266240032, 0.9315713144341328, 0.7147630354913195, 0.5567298984316931, 0.4802819394543402, 0.435069173957337, 0.3554531828786661, 0.3329021548363275, 0.28907186473492813]},
			"USAC_PROSAC": {"F": [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000], "angle_error": [3.8073164171421707, 1.6237722722945531, 1.2543793858159993, 0.7972119287197754, 0.6360907409058727, 0.5303098535762839, 0.47998498685189056, 0.39761124668923037, 0.3662159175230347, 0.32238610847061655]}
		};

		// Color scheme for different methods
		const reconstructionColors = {
			"RANSAC": "#FF6384",
			"7Point": "#36A2EB",
			"8Point": "#FFCE56",
			"USAC_DEFAULT": "#4BC0C0",
			"USAC_ACCURATE": "#9966FF",
			"USAC_MAGSAC": "#FF9F40",
			"USAC_PROSAC": "#C9CBCF"
		};

		// Generate upper half of ellipsoid surface
		function generateUpperEllipsoidSurface(a, b, c, resolution = 50) {
			const phi = [];
			
			for (let i = 0; i <= resolution; i++) {
				phi.push(i * 2 * Math.PI / resolution);
			}

			const x = [];
			const y = [];
			const z = [];

			// Generate only upper half: theta from 0 to PI/2
			const thetaSteps = resolution / 2;
			for (let t = 0; t <= thetaSteps; t++) {
				const thetaSlice = t * (Math.PI / 2) / thetaSteps;
				const xSlice = [];
				const ySlice = [];
				const zSlice = [];
				
				for (let p = 0; p < phi.length; p++) {
					const phiSlice = phi[p];
					xSlice.push(a * Math.sin(thetaSlice) * Math.cos(phiSlice));
					ySlice.push(b * Math.sin(thetaSlice) * Math.sin(phiSlice));
					zSlice.push(c * Math.cos(thetaSlice));
				}
				x.push(xSlice);
				y.push(ySlice);
				z.push(zSlice);
			}

			return { x, y, z };
		}

		// Generate random points on upper half of ellipsoid surface
		function generateRandomPointsOnUpperEllipsoid(a, b, c, numPoints) {
			const x = [];
			const y = [];
			const z = [];

			for (let i = 0; i < numPoints; i++) {
				// Generate random spherical coordinates for upper half
				const u = Math.random() * Math.PI / 2;  // theta: 0 to PI/2 (upper half)
				const v = Math.random() * 2 * Math.PI;  // phi: 0 to 2PI
				
				// Convert to ellipsoid coordinates
				x.push(a * Math.sin(u) * Math.cos(v));
				y.push(b * Math.sin(u) * Math.sin(v));
				z.push(c * Math.cos(u));
			}

			return { x, y, z };
		}

		// Create or update the ellipsoid plot
		function createEllipsoidPlot(a, b, c) {
			// Generate upper half ellipsoid surface
			const surface = generateUpperEllipsoidSurface(a, b, c);

			// Generate random points (200 points) on upper half
			const randomPoints = generateRandomPointsOnUpperEllipsoid(a, b, c, 200);

			// Create traces for Plotly
			const surfaceTrace = {
				type: 'surface',
				x: surface.x,
				y: surface.y,
				z: surface.z,
				colorscale: [[0, 'pink'], [1, 'pink']],
				opacity: 0.7,
				showscale: false,
				hovertemplate: 'X: %{x:.2f}<br>Y: %{y:.2f}<br>Z: %{z:.2f}<extra></extra>'
			};

			const pointsTrace = {
				type: 'scatter3d',
				mode: 'markers',
				x: randomPoints.x,
				y: randomPoints.y,
				z: randomPoints.z,
				marker: {
					size: 5,
					color: 'blue',
					opacity: 0.9,
					line: {
						color: 'rgba(0,0,0,0.3)',
						width: 1
					}
				},
				name: 'Random Points',
				hovertemplate: 'Point<br>X: %{x:.2f}<br>Y: %{y:.2f}<br>Z: %{z:.2f}<extra></extra>'
			};

			// Layout configuration
			const layout = {
				scene: {
					xaxis: { title: 'X', range: [-a, a] },
					yaxis: { title: 'Y', range: [-b, b] },
					zaxis: { title: 'Z', range: [0, c] },
					aspectmode: 'data'
				},
				margin: { l: 0, r: 0, t: 0, b: 0 },
				paper_bgcolor: 'rgba(0,0,0,0)'
			};

			// Create or update the plot
			const data = [surfaceTrace, pointsTrace];
			Plotly.react('ellipsoidPlot', data, layout, { responsive: true });
		}

		// Update plot with new values from inputs
		function updateEllipsoidPlot() {
			const a = parseFloat(document.getElementById('axisA').value);
			const b = parseFloat(document.getElementById('axisB').value);
			const c = parseFloat(document.getElementById('axisC').value);

			// Validate inputs
			if (isNaN(a) || isNaN(b) || isNaN(c) || a <= 0 || b <= 0 || c <= 0) {
				alert('Please enter valid positive numbers for all axes.');
				return;
			}

			createEllipsoidPlot(a, b, c);
		}

		// Initial plot creation with default values
		createEllipsoidPlot(9.0, 14.0, 6.0);

		// Allow Enter key to trigger update
		document.querySelectorAll('.controls input').forEach(input => {
			input.addEventListener('keypress', function(e) {
				if (e.key === 'Enter') {
					updateEllipsoidPlot();
				}
			});
		});

		// Reconstruction Error Plots Functions
		function createReconstructionVerticalLineShapeAndAnnotation(focalLength, phoneName) {
			if (!focalLength) return { shapes: [], annotations: [] };
			return {
				shapes: [{
					type: 'line',
					x0: focalLength,
					x1: focalLength,
					y0: 0,
					y1: 1,
					yref: 'paper',
					line: {
						color: 'rgb(0, 0, 0)',
						width: 4,
						dash: 'dashdot'
					},
					name: 'Focal length'
				}],
				annotations: [{
					x: focalLength,
					y: 1,
					yref: 'paper',
					text: phoneName,
					showarrow: false,
					xanchor: 'center',
					yanchor: 'bottom',
					yshift: 10,
					font: {
						color: 'rgb(0, 0, 0)',
						size: 14,
						family: 'Arial, sans-serif'
					},
					bgcolor: 'rgba(255, 255, 255, 0.8)',
					bordercolor: 'rgb(0, 0, 0)',
					borderwidth: 1,
					borderpad: 5
				}]
			};
		}

		function getReconstructionPositionTraces() {
			const methods = ["RANSAC", "7Point", "8Point", "USAC_DEFAULT", "USAC_ACCURATE", "USAC_MAGSAC", "USAC_PROSAC"];
			const traces = [];
			
			methods.forEach(method => {
				if (document.getElementById(`pos-${method}`).checked) {
					traces.push({
						x: positionErrorsData[method].F,
						y: positionErrorsData[method].pos_error,
						mode: 'lines+markers',
						name: method,
						line: {
							color: reconstructionColors[method],
							width: 2
						},
						marker: {
							size: 6
						}
					});
				}
			});
			
			return traces;
		}

		function getReconstructionAngleTraces() {
			const methods = ["RANSAC", "7Point", "8Point", "USAC_DEFAULT", "USAC_ACCURATE", "USAC_MAGSAC", "USAC_PROSAC"];
			const traces = [];
			
			methods.forEach(method => {
				if (document.getElementById(`ang-${method}`).checked) {
					traces.push({
						x: angleErrorsData[method].F,
						y: angleErrorsData[method].angle_error,
						mode: 'lines+markers',
						name: method,
						line: {
							color: reconstructionColors[method],
							width: 2
						},
						marker: {
							size: 6
						}
					});
				}
			});
			
			return traces;
		}

		function updateReconstructionPositionPlot() {
			const selectedPhone = document.getElementById('phone-select').value;
			const focalLength = reconstructionPhoneFocalLengths[selectedPhone];
			const { shapes, annotations } = createReconstructionVerticalLineShapeAndAnnotation(focalLength, selectedPhone);
			
			const traces = getReconstructionPositionTraces();
			const layout = {
				title: {
					text: 'Error of 3d reconstruction',
					font: { size: 16 }
				},
				xaxis: {
					title: 'Focal length, pixels',
					range: [0, 5500]
				},
				yaxis: {
					title: 'Position error, mm',
					range: [0, 4]
				},
				shapes: shapes,
				annotations: annotations,
				showlegend: true,
				legend: {
					x: 0.02,
					y: 0.98
				},
				hovermode: 'closest'
			};

			Plotly.react('reconstructionPositionChart', traces, layout, { responsive: true });
		}

		function updateReconstructionAnglePlot() {
			const selectedPhone = document.getElementById('phone-select').value;
			const focalLength = reconstructionPhoneFocalLengths[selectedPhone];
			const { shapes, annotations } = createReconstructionVerticalLineShapeAndAnnotation(focalLength, selectedPhone);
			
			const traces = getReconstructionAngleTraces();
			const layout = {
				title: {
					text: 'Error of found rotation',
					font: { size: 16 }
				},
				xaxis: {
					title: 'Focal length, pixels',
					range: [0, 5500]
				},
				yaxis: {
					title: 'Rotation angle error, degrees',
					range: [0, 20]
				},
				shapes: shapes,
				annotations: annotations,
				showlegend: true,
				legend: {
					x: 0.02,
					y: 0.98
				},
				hovermode: 'closest'
			};

			Plotly.react('reconstructionAngleChart', traces, layout, { responsive: true });
		}

		function updateReconstructionPhoneSelection() {
			updateReconstructionPositionPlot();
			updateReconstructionAnglePlot();
		}

		// Initialize reconstruction plots when page loads
		updateReconstructionPhoneSelection();
	</script>

	<hr />
	<p style="text-align:center">Samsung R&D Institute Ukraine, Copyright&nbsp;&copy; 2026</p>
  </body>
</html>
